<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Present Day, Present Time</title>
    <link>https://gobomb.github.io/post/</link>
    <description>Recent content in Posts on Present Day, Present Time</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Apr 2022 16:59:44 +0800</lastBuildDate><atom:link href="https://gobomb.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weave 和 Flannel 的区别</title>
      <link>https://gobomb.github.io/post/weave-and-flannel/</link>
      <pubDate>Sat, 09 Apr 2022 16:59:44 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/weave-and-flannel/</guid>
      <description>这篇文章主要讨论 weave 和 flannel 各自怎么使用 vxlan 实现跨主机网络的情况。从网络设备、主机路由、IP管理、CNI实现、容器路由、vxlan发现几个角度来比较。
1. weave 有哪些网络设备 一个 bridge ，通常命名为 weave，会配一个 IP。用于挂载和容器成对的 veth。
一个 ovs 设备，命名为 datapath，没有配 IP。
一个 vtep 设备 vxlan-6784，负责封包和拆包，没有配 IP。
bridge 和 ovs 设备之间用一对 veth 连起来： vethwe-datapath 和 vethwe-bridge。
主机路由和网段划分 只有一条路由，所有 pod 是一个子网，不区分 node，IPAM 由weave 主机分配， weave cni 会调用 weave 服务获取 IP。
# 在 node 上执行 ip route 10.32.0.0/12 dev weave proto kernel scope link src 10.32.0.1 10.32.0.0/12 是该集群的 pod CIDR。
CNI 阶段 kubelet 调用 weave-plugin 这个 cni 插件，通过 weave client 从 weave daemon 获取 ip（ip分配由weave管理），并在容器内配置网卡和 IP，并将网卡对应的 veth 连接到 weave bridge 上。</description>
    </item>
    
    <item>
      <title>Golang Sync Map</title>
      <link>https://gobomb.github.io/post/golang-sync-map/</link>
      <pubDate>Mon, 04 Apr 2022 22:33:49 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/golang-sync-map/</guid>
      <description>适用的情形是 1. 读多写少 2. 多个g读写的key不同。
使用冗余的 map，来分离读写，相当多了一个缓存。两个map分别为dirty、read，他们的value存的是指针，相当于value是共享的，节省一些内存。
type Map struct { // 保护dirty、misses，read的提升过程 	mu Mutex // 如果是read中的已知key，不需要加锁 	read atomic.Value // 新加的key会写到dirty中 	dirty map[interface{}]*entry // 记录从read未命中，到dirty中去查的次数 	misses int } // read对应的数据结构 type readOnly struct { m map[interface{}]*entry amended bool } // 表示value的指针 type entry struct { p unsafe.Pointer } 对dirty的增删查改需要加锁，对read的更新、读、删除则不需要，使用原子操作实现。所以这里性能比一般的map+mutex更好的地方，在于原子操作比mutex快。
func (m *Map) Load(key interface{}) (value interface{}, ok bool) { read, _ := m.read.Load().(readOnly) e, ok := read.</description>
    </item>
    
    <item>
      <title>Golang Mutex</title>
      <link>https://gobomb.github.io/post/golang-mutex/</link>
      <pubDate>Mon, 04 Apr 2022 00:04:19 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/golang-mutex/</guid>
      <description>有两种模式：正常模式和饥饿模式。正常模式会先自旋再通过信号量排队等待；饥饿模式会严格按照先入先出的顺序。
  为什么需要有两种模式？golang的锁是通过自旋+信号量的方式实现的。g获取锁需要先自旋一定次数，再放到信号量等待。当锁释放，会唤醒信号量队头的g。这个g会和新来的g产生竞争，新来的g处于自旋状态，更容易抢到锁，队列中的g会被饿死。性能虽高，但是公平性低。所以引入了饥饿模式解决饥饿问题。
  为什么后来者有优势？
 后来者在cpu上自旋，更有优势 被唤醒的g每次只有一个，自旋的有很多    数据结构：由 state 和 sema 组成，零值可用。零值表示解锁状态。
sema表示信号量
state低三位，locked表示是否加锁，woken表示是否有g唤醒，starving表示是否进入饥饿模式。这3位以外的位表示有多少g在排队
  加/解锁首先会进入 fastpath，原子操作改变 state。如果没有成功再进入 slowpath
// 加锁 if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) { // 直接对释放的锁加锁成功 return } m.lockSlow() // 解锁 new := atomic.AddInt32(&amp;amp;m.state, -mutexLocked) // new等于0意味着没有g在排队 if new != 0 { m.unlockSlow(new) }   lockSlow   先进入自旋，然后判断是否进入饥饿模式，把g放到信号量中等待
  进入自旋：
 woken置为1 ，防止唤醒太多g do_spin()自旋，调用30次 PAUSE 指令    g进入自旋的条件：</description>
    </item>
    
    <item>
      <title>Basic Paxos 总结</title>
      <link>https://gobomb.github.io/post/basic-paxos/</link>
      <pubDate>Tue, 22 Mar 2022 22:46:11 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/basic-paxos/</guid>
      <description>Paxos 解决什么问题？
对于一个多节点的分布式系统，客户端写入一个值，得到ok的返回之后，希望这个值是不变的、持久化的。而且期待不管从哪个节点读，都是一致的。
复制解决什么问题？
高可用性：冗余数据，防止单点故障
降低延迟：把数据复制到不同的地理位置，方便客户端更快地读取
伸缩性：提高机器副本从而提高吞吐量
几种复制方式 1 主从异步复制 主节点收到客户端的写请求，把数据落盘，然后响应客户端ok，再把数据复制到其他节点。
问题：响应ok的时刻，从节点总是落后于主节点的，加入主节点当机，客户端的写入就丢失了，因为从节点没来得及写入副本。
2 主从同步复制 主节点收到客户端的写请求，把数据落盘，把数据复制到其他节点，得到从节点的响应后，再响应客户端ok。
问题：主节点会阻塞在从节点的处理上，一方面性能会下降，需要等待网络以及从节点的写盘，另一方面，当从节点当机，主节点就不可用了。
3 主从半同步复制 主节点收到客户端的写请求，把数据复制到足够多的机器上，而不是全部，得到部分从节点的确定后，再响应客户端ok。
问题：有可能从库没有完整的数据
4 多数派读/写 每条数据写入到半数以上的节点上，读也是要读半数以上的节点。
假设有3个节点 node-1,node-2,node-3，客户端需要写入到至少2个节点才算成功。
多数派读/写的问题 1 两次写入到不同的2个节点，其中有一个节点会被覆盖 比如
// 对 node-1, node-2 都写入了a=x node-1，a=x;node-2,a=x;node-3,null // 第二次对node-2, node-3写入了a=y node-1，a=x;node-2,a=y;node-3,a=y // 读 node-1，node-2 ,从 node-1 得到 a=x，从 node-2 得到 a=y，无法确定哪条数据是对的。 解决：使用全局时间戳，写入的时候对每个数据设置一个全局递增的时间戳或者版本号。已更新的时间戳或更大的版本号为准，也就是 最后写入胜利（LWW）。
node-1 , a=null , v0; node-2 , a=null, v0; node-3 , a=null , v0; // 对 node-1, node-2 都写入了a=x node-1 , a=x , v1; node-2 , a=x , v1; node-3 , a=null , v0; // 第二次对node-2, node-3写入了a=y node-1 , a=x , v1; node-2 , a=y , v2; node-3 , a=y , v2; // 读 node-1，node-2 ,从 node-1 得到 a=x，v1 从 node-2 得到 a=y，v2。v2&amp;gt;v1，所以 a=y 是正确的 2 单个客户端写的原子性难以保证 因为客户端需要写多个节点，有可能写的过程中客户端崩溃，没有写入超过半数，系统里的数据会产生不一致。</description>
    </item>
    
    <item>
      <title>Informer Resync 做了什么</title>
      <link>https://gobomb.github.io/post/whats-resync-in-informer/</link>
      <pubDate>Sat, 19 Feb 2022 01:42:09 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/whats-resync-in-informer/</guid>
      <description>resync 不是 re-list。
  resync 是重放 informer 中的 obj 到 DeltaFIFO 队列中，触发 handler 再次处理 obj。
目的是防止有些 handler 处理失败了而缺乏重试的机会。特别是，需要修改外部系统的状态的时候，需要做一些补偿的时候。
比如说，根据 networkpolicy刷新 node 上的 iptables。
iptables 有可能会被其他进程或者管理员意外修改，有 resync 的话，才有机会定期修正。
这也说明，回调函数的实现需要保证幂等性。对于 OnUpdate 函数而言，有可能会拿到完全一样的两个 Obj，实现 OnUpdate 时要考虑到。
  re-list 是指 reflector 重新调用 kube-apiserver 全量同步所有 obj。但目前(v1.20)没有显式配置 re-list 周期的参数。
官方开发者似乎觉得不需要 re-list，目前的方式已经足够可靠
list 的时机一般是在程序第一次启动，或者 watch 有错误，才会 re-list。
具体什么时候会发生 re-list？这里有篇博客可以参考
  如何配置 resync 的周期？
func NewSharedIndexInformer(lw ListerWatcher, exampleObject runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers)</description>
    </item>
    
    <item>
      <title>sharedInformer 如何实现注册多组 ResourceEventHandler</title>
      <link>https://gobomb.github.io/post/how-sharedinformer-register-multi-resource-event-handler/</link>
      <pubDate>Thu, 17 Feb 2022 20:00:57 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/how-sharedinformer-register-multi-resource-event-handler/</guid>
      <description>client-go 的 cache 包缓存对象数据的流向是：kube-apiserver-&amp;gt; reflector -&amp;gt; DeltaFIFO -&amp;gt; infomer 里的 Indexer，同时 call 用户实现的 ResourceEventHandler：
 reflector从apiserver list全量数据，并watch 后续数据的更新，会定期进行一次全量 list，防止本地缓存与服务端不一致 reflector 会将 obj 写到 DeltaFIFO 中 DeltaFIFO 可以定时（设置了resyncPeriod的话）从 Indexer 取 key，重新入队，防止一旦 obj 在 handler 里计算失败，得不到重试的机会 controller 的 processLoop 循环调用 Pop 和 Process 函数， Pop 从 DeltaFIFO 取 Obj ，Process 更新 Indexer，并触发 ResourceEventHandler  普通 Informer 对同一种对象只能注册一组 ResourceEventHandler，要实现多组 ResourceEventHandler，就需要多创建 Informer，数据是一样的，会造成内存上的浪费。所以 sharedInformer 的引入，支持了针对同一类对象的 ResourceEventHandler 注册到一个 informer 上，实现数据的复用。
本文主要从源码的层面，分析一下这个多组 ResourceEventHandler 的注册是怎么实现的，数据是怎样做分发的。
为了实现 sharedindexinformer 挂载多组 handler，相对普通 informer，有两点不同：一是重写了Process方法，用HandleDeltas方法替代；二是引入两个新的结构：sharedProcessor、processorListener，用于做数据分发。</description>
    </item>
    
    <item>
      <title>告别哲学</title>
      <link>https://gobomb.github.io/post/say-goodbye-to-philosophy/</link>
      <pubDate>Tue, 01 Feb 2022 04:09:11 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/say-goodbye-to-philosophy/</guid>
      <description>这两年受维特根斯坦和陈嘉映老师影响很深。第一，对哲学的一些基本看法改变了，重新梳理了自己和哲学的关系；第二，对人生的很多基础假设也改变了，包括一些根深蒂固影响了十几年的假设。这个改变不是“瓦解”，也不是“突变”，而是“自然生长”。
我是从初中二年级开始接触哲学的（实实在在的中二少年），从一本教辅性质的书中的一篇《苏菲的世界》节选开始。之前模模糊糊知道苏格拉底是大哲学家，却不知道这个人的具体学说和人格。这篇文章通过浅显易懂的对话体形式介绍苏格拉底，勾起了我的兴趣。后来在书店里购得此书，手不释卷看了一周。哲学史上环环相扣的“正反合”路径，看得让我异常兴奋。
读到贝克莱主教彻底的唯心主义时，加上小说讨巧的叙事方式的影响，我的“世界观”被颠覆了一遍。按照我朴素的思考能力，其他哲学家的学说，我或多或少都可以论证或反驳一番，换言之，这些学说都还能被容纳进我旧有的框架。但是读到贝克莱，我框架底部支撑性的观念一下子就被抽走了，自以为还算牢靠周密的理解框架，就像抽掉关键支柱的积木，瞬间崩塌。我感到持续的兴奋、惊讶、害怕，还有晕眩，就像发烧一样。这一烧烧了十几年。
哲学从此成为我阅读和（精神）生活里的一大板块。但这里不打算细谈心路历程，主要写写最近最新的结论：我打算把哲学的优先级降到较低的位置。为什么呢？
 维特根斯坦指出了研究哲学问题的误导之处。柏拉图认为人必须要到30岁后才能学习哲学。在没有足够的生活经验作为材料时，是谈不上哲学思考的，脱离实际经验的思考是低效、空转的。 作为年轻人，多去经历生活，多去实践，多去尝试具体的事情，是更重要的。很多道理不是从纯粹思辨中推倒出来的，而是从行动、生活、具体经验中来的。广阔天地，大有可为。 过去花了太多精力在哲学思考上面了，错失了成长中必要的锻炼。以为思考能替代一些其他的付出。走了弯路，现在回头还来得及。这个兴趣也会分散我的精力，给我的职业发展带来负面影响，还影响我搞钱。 哲学思考是需要物质条件保证的。我不是富二代，也还没有财务自由，我得承认现实，并且要承担起现实的责任。 我希望发展我其他的能力，人际交往的能力，规划的能力，执行力，让自己发展得更成熟。 我也学够了。基本上对这个领域，作为业余爱好者，好奇的部分基本都了解过了，现在也没有什么遗憾。我挺清楚自己的资质和上限所在。对职业化的哲学也没有很热衷。  我从小爱好思考。爱好思考这个特质，使得我与哲学结缘。爱好思考不见的就是个好事。因为凡事应有度，过犹不及。由于缺乏好的教导和指引，过去我太倚重思考，并得到一些惨痛的教训。
为什么爱好思考不一定就好呢？
  在不适合思考的场景思考，就不好（这听起来真像是一句正确的废话，典型的同义反复，“不适合”就蕴含着“不好”了）。比如对情绪问题，应该好好感受，而不是思考；比如该行动的时候，思考得再周全而不做事对结果并没有帮助；比如对沟通问题，应该倾听他人说话，耐心沟通，而不是空想、臆测。有时一个人想破天还不如好好观察他人，或者一句发问来得简单直接。
  思考得多，会抑制其他活动，导致失衡。思考是能带来乐趣的，但沉浸在这类型的乐趣不愿出来，也是个蛮大的毛病，算是一种“偏食” 。可以多元一些，多接触一下其他有意思的乐趣。
  当然，学习哲学、爱好思考还是给我带来一些好处：
 培养了直觉思维，对（文科的）抽象概念比较有感觉。 使得心态比较开放，愿意去了解异见。 对很多人文学科，有一个相对稳定的立场和框架，去学习和吸收。  哲学思考是一种冲动，有冲动不见得就有所建树。别“误把创作欲当作创作才华“。我希望少花点时间在哲学和空想上面，多做点有反馈、有产出、有具体用处的事情。如果真有思考、表达的冲动，那就写成完整的文章吧，勉强算得上是有形的产出。</description>
    </item>
    
    <item>
      <title>给 Kubernetes 提 PR</title>
      <link>https://gobomb.github.io/post/k8s-pr/</link>
      <pubDate>Mon, 12 Oct 2020 02:02:25 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/k8s-pr/</guid>
      <description>之前在解决 weave 的问题时，顺便也发现了 K8s client-go 代码中存在的问题 ，并提交了 PR ，九月份的时候终于被 merge 进 1.20 的 release，并 cherry pick 回 1.17～1.19（不过公司使用的环境是 1.16，社区不再提供官方的补丁……）
weave 因为依赖的这个库的 bug，掩盖了本应该很快发现的 bug，从而导致很多诡异的现象，我觉得这个库应用这么广泛，应该有不少开发者也受影响了才对。而且我和同事在其他项目的使用过程中，也发现过类似的现象（Process 被阻塞，不能正确处理新的事件）。但一开始在 k8s 的 issues 翻了很久，都找到没有类似的问题，实在是违反直觉。
后来仔细看了下 client-go 源码，也找到了合理的解释。informer 有两种用法，一种是使用 client-go/tools/cache/controller.go 中提供的 controller（可以说是低级别的 informer），一种是 client-go/tools/cache/shared_informer.go 中的 SharedInformer 或者 SharedIndexInformer，也就是 SharedInformerFactory 返回的 informer。 SharedIndexInformer 封装的层次更高，大部分项目都是用的后者，刚好避开了普通 informer 存在的 bug。而我们的部分项目，以及 weave-npc 都是用的 controller/低级别informer。
具体而言，bug 就是刚好发生在 controller/低级别informer 和 SharedIndexInformer 对 Contorller 接口方法 Run() 的不同实现上面。它们对 ResourceEventHandler 接口的处理方式或者说 Process 的实现是不同的，而 ResourceEventHandler 的实现就是用户业务代码发生 panic 的地方。</description>
    </item>
    
    <item>
      <title>K8s client-go 源码阅读笔记</title>
      <link>https://gobomb.github.io/post/k8s-client-go-note/</link>
      <pubDate>Mon, 12 Oct 2020 00:53:25 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/k8s-client-go-note/</guid>
      <description>我们可以通过 list 和 watch 的机制向 k8s 的 apiserver 同步 k8s 资源对象和对象的变化。client-go 是官方提供的 apiserver go 客户端库，封装了对 k8s 对象的操作，实现了一个 apiserver 的缓存。这篇笔记主要在 interface 的层面介绍一下 cache 包的结构。interface 构成不同业务逻辑的边界，调用往往都通过 interface 中的方法来进行，作为调用方只需要关心抽象行为，而不用关心具体实现。通过学习 client-go 的代码，也可以了解 interface 的最佳实践。从 interface 入手，可以快速了解整套代码的骨架。
client-go 实现了一个缓存，定期同步和 apiserver / etcd 中的数据，并且提供了一组响应资源变化的回调函数。作为库的用户只要实现回调函数，处理自己关心的资源对象（如Pod、Deployment、自己实现的 CR 等等），而不需要关心和 apiserver 具体的交互逻辑（list and watch）。相当于一个编程框架，让开发者专注于自己的业务逻辑而不是重复实现通用的轮子。
以下通过几个重要的概念来介绍 client-go cache 包（位于 kubernetes/staging/src/k8s.io/client-go/tools/cache/ 基于 1c5be7dd5046fba8733f44618fd28fbb79e7db07 ）的机制，名词后面的括号说明这个名词是interface还是struct：
  ClientSet（struct）：封装了 k8s 所有资源对象的获取方式，实现了每一种对象的 list 和 watch 接口，可以通过 http 跟 apiserver 交互。实现了 ListerWatcher 接口。
  ListerWatcher（interface）：</description>
    </item>
    
    <item>
      <title>给 WeaveNet 社区提 PR</title>
      <link>https://gobomb.github.io/post/debug-and-pr-for-weave/</link>
      <pubDate>Sun, 16 Aug 2020 00:45:01 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/debug-and-pr-for-weave/</guid>
      <description>任职的公司 K8s 集群网络插件用的是 WeaveNet。WeaveNet 是一套基于 VXLAN 的容器网络方案。我们还用了 weave-npc 作为 K8s NetworkPolicy 的实现。从 2.6.0 版本开始，weave 存在一个很严重的 bug，会导致新创建的 pod 所有的网络流量被丢弃，在我们开发、测试、生产环境 K8s 集群里有很大的概率被触发。本文主要记录如何定位到该 bug 以及如何修复，给 weave 上游提 Pull Request 并被接受的过程。
现象及复现 K8s 集群使用 weave-kube 和 weave-npc 2.6.0～2.6.5 版本。创建一个 namespace，在此 namespace 下创建 2 到 3 个 deploymnet，同时每个 deployment 创建一个 networkpolicy，允许网络流量的进出。在这种配置下，该 deployment 关联的 pod 是能够访问到 networkpolicy 放行的 IP 的。
在一切部署完成后，删除该 namespace kubectl delete namespace [ns name] 。重复此操作 n 次，会发现，新创建的 pod 的所有网络访问都不通了，包括 networkpolicy 所允许的。
weave-kube 和 weave-npc 都是作为 daemonset 部署的。查看 pod 所在 node 上的 weave-npc 的日志，可以看到大量的 pod 对应的流量被 block 的日志。</description>
    </item>
    
    <item>
      <title>2019 年终总结</title>
      <link>https://gobomb.github.io/post/2019-summary/</link>
      <pubDate>Sun, 29 Mar 2020 01:37:25 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/2019-summary/</guid>
      <description>打字的时刻是2020年3月28日，2020都已经过掉了4个月了，我才厚着脸皮写2019的总结。
多多少少有回避的心态。但不管怎样，这是个仪式，总要和过去的一年道个别，然后在下一年好好改进。
2019的关键词是成长。虽说也还没有肉眼可见的成果：没有真正拿得出手的项目，没有引以为傲的作品，财务上也没有值得一提的增长。这个成长更多体现心态和性格上。发生了不少重要的事件，让我对生活、工作、自我认知的态度有了深刻的转变。
工作上，主要侧重于计算机网络。这是我向来感兴趣的领域，也有幸在工作中得到相关的锻炼：任职的公司有相应的场景可以摸索，能跟随技术很强的mentor学习。怎么去定位网络问题，怎么去使用相关的工具，怎么去做网络插件的开发，慢慢开始入门。
对容器相关的技术也有了更进一步的了解，但还不够深入。需要阅读的源码，需要理清楚的概念，我还是没有付出足够的时间的。也体现在：没办法深入浅出地向别人介绍这些知识，当被提出质问的时候，会特别不自信。我越发觉得如果对一项技术没有自信心，就说明没有真正吃透。很遗憾，在很多方面，我都是处于没自信的状态。
也写了几篇博客，准确来说是写了几篇学习笔记。写博客最大的焦虑就是，认为自己写的东西没有用。毕竟互联网重复的信息那么多，大家时间那么少，凭什么来看你的东西呢？我又何必增添噪音？我的想法是，我就是写写学习笔记嘛，我是总结自己的学习过程、记录自己的思考而已，如果对读者有用自然是好事，没有的话，至少对我自己是有用的，这是个备忘，我随时可以查阅。写博客这回事，向draveness大佬学习，总是没错的。他的高产和质量自然是很难做到，但努力做到他的十分之一，也还是值得追求的。
 容器运行时笔记 ：对容器runtime概念的梳理，总体还是比较凌乱，有堆砌信息的嫌疑，在写的时候是知道存在这些缺点的，还是硬着头皮写下去。先写出来再慢慢修改，比起想着缺点不愿意写，总是更好的（写代码也是一样的道理）。另外容器的发展史和生态确实比较凌乱，有技术的原因，有商业的原因，要清晰地讲清楚那些概念的渊源，还是有点超出我能力的。后来我发现我能读懂 cmgs 的文章了，18年还没毕业的时候看《容器战争》 就是一种不明觉厉的心态。 Kubernetes 网络学习：阅读 Flannel 源码 ：k8s 网络插件 flannel 的源码阅读笔记。这篇对我还是蛮重要的，踏出网络插件学习的第一步。选择 flannel 是因为它比较简单，而我自己私下搭的集群也是用的这个。读源码的收获还是蛮大的。其实各大插件的大同小异，看懂了一个的套路，其他的也可以举一反三，对我在工作中定位问题帮助很大。 通过实验学习 Linux VETH 和 Bridge ：对 Linux 虚拟网络设备的学习笔记。让我对 Linux 内核以及网络栈有了感性的认识。以前学计算机网络是从理论和设计的角度去理解，Linux 网络设备拓展了我看网络的角度：应用/软件的角度。这篇博客还是写得挺愉快的，一个是理论能在应用中得到应征，另一个是平常工作学到的一些技巧也能用上。挺有成就感的。 Wireguard 使用笔记 ：Wireguard 是比较新的 VPN 软件，是少数能被 Linus 表扬的代码（“it&amp;rsquo;s a work of art”），已经合并入5.6内核。所以我一直有好感。这篇博客的由头，也很值得一提：是一个朋友在使用 Wireguard 的时候遇到了问题，然后我们一起讨论解决了问题。这种一起工作分析解决问题的感觉超棒！这个时候从应用的角度，我对二/三层网络、iptables、tcpdump 都比较熟悉了，也是一个对自己知识的检验。文章写得还是比较流水账，不怎么突出重点，还是有进步的空间的。 Kubernetes 网络学习：阅读 ovn-kubernetes 源码 ：由于工作的关系，稍微接触了 SDN 以及 OVN。这又是一个大坑。花了不少时间和精力去阅读 OVN 和 OVS 并不好读的文档，再一次处于每个单词都知道但就是读不懂的尴尬境地（可能又一半的时间是耗费在自我怀疑上面）。ovn-kubernetes 是尝试将 OVN 的体系引入 k8s 作为网络插件，之前对 flannel 的接触至少让我不需要对 k8s 的“套路”有额外的负担。对于我这个不喜欢折腾硬件更喜欢软件/虚拟化的人而言，OVN 还是蛮有意思的。见识了 OVN 的架构，也让我强化了某种错觉：分布式软件似乎都大同小异，OVN / k8s / Hadoop 其实都有其共同之处？另一个让我不知道是好还是坏的点是：OVN / OVS，其实是绕开了 Linux 网络栈，对 Linux 网络的那些经验，其实挺难迁移过去的。不得不学习一系列新的协议和工具，没法用 tcpdump 一把梭了……（学不动了  coding 方面：做了一些网络配置方面的工作（跨集群网络的设置、k8s 访问控制策略的实现），做了部分重构。很感谢 mentor 对我的耐心指导（虽然他在2020离职了）。另外写了一些临时的代码，对个人 coding 能力并没有很大的帮助。这方面一直也有很多困惑：代码还是写得太少了，一方面是基础架构的功能迭代确实不频繁，另一个方面是创业公司的需求快速变化，很难有完整系统实践的机会。我很担心这个会成为我的短板，我可能运维或者定位线上问题的能力还行，但代码量是远远比不上搞业务的同学的。</description>
    </item>
    
    <item>
      <title>使用 OpenSSH 建立 L2 和 L3 隧道</title>
      <link>https://gobomb.github.io/post/build-l2-l3-vpn-by-openssh/</link>
      <pubDate>Thu, 09 Jan 2020 12:05:28 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/build-l2-l3-vpn-by-openssh/</guid>
      <description>服务器环境 server：ubuntu，处于172.16.0.0/16网段，默认网卡是eth0，server处于公网 client：centos，处于10.10.13.0/24网段，默认网卡是ens192，client处于nat之后 server端的配置 vi /etc/ssh/sshd_config
PermitRootLogin yes PermitTunnel yes L3 Tunnel 配置过程 在client上执行
ssh -w 5:5 root@[server 公网ip] 如果-w设置为any，会自动使用下一个可用的tun设备。
执行完该命令会进入到server中，client和server各自会创建一个link：tun5。
给server的VPN网卡配置地址和路由：
# 配置地址10.0.1.1 # 或者 ip address add 10.0.1.1/30 dev tun5 ifconfig tun5 10.0.1.1 netmask 255.255.255.252 # 启动网卡 ip link set tun5 up # 配置到达10.10.13.0/24的路由 # 或者 # ip route add 10.10.13.0/24 dev tun5 route add -net 10.10.13.0 netmask 255.255.255.0 dev tun5 # 开启转发 echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward iptables -t filter -A FORWARD -P ACCEPT # 设置nat iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 在client：</description>
    </item>
    
    <item>
      <title>Kubernetes 网络学习：阅读 ovn-kubernetes 源码</title>
      <link>https://gobomb.github.io/post/learning-k8s-networking-reading-ovn-kubernetes-source/</link>
      <pubDate>Sun, 29 Dec 2019 22:37:00 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/learning-k8s-networking-reading-ovn-kubernetes-source/</guid>
      <description>1. 软件定义网络介绍  何谓软件定义网络（SDN，software-defined networking）
根据维基百科的定义：软件定义网络是一种新型网络架构。它利用OpenFlow协议将路由器的控制平面（control plane）从数据平面（data plane）中分离，改以软件方式实现。该架构可使网络管理员在不更动硬件设备的前提下，以中央控制方式用程序重新规划网络，为控制网络流量提供了新方案，也为核心网络和应用创新提供了良好平台。
在传统网络中，我们要修改整个网络的配置，可能需要去每一台路由器上面更改路由配置；而在 SDN 中，只需要修改 SDN 的集中控制器，配置的更改会被控制器通过统一的协议下发到路由器上。这里路由器是实际数据转发的节点，但路由信息的配置转移到了集中控制器上，使得更改网络配置更加灵活。
2. OVN (Open Virtual Network)介绍  Open vSwitch（OVS）是一款开源的分布式虚拟交换机，可以理解成软件实现的交换机。而 OVN 是基于 OVS 实现的一套网络方案，可以虚拟出二层和三层的网络。
OVN 的架构图如下：
 CMS | | +-----------|-----------+ | | | | OVN/CMS Plugin | | | | | | | | OVN Northbound DB | | | | | | | | ovn-northd | | | | +-----------|-----------+ | | +-------------------+ | OVN Southbound DB | +-------------------+ | | +------------------+------------------+ | | | HV 1 | | HV n | +---------------|---------------+ .</description>
    </item>
    
    <item>
      <title>Wireguard 使用笔记</title>
      <link>https://gobomb.github.io/post/wireguard-notes/</link>
      <pubDate>Mon, 07 Oct 2019 16:40:14 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/wireguard-notes/</guid>
      <description>Situation 公司和家庭在各自的内网里，并不互通，使用 wireguard 加一台处于公网的服务器，组成虚拟局域网，方便在家里访问公司内网或在公司访问家庭内网。
为什么不用 frp 或端口映射工具？ 需求之一是希望七层应用（包括 DNS）都不需要特别地设置；而端口映射作用在四层，每当有一个新的应用都得配置一遍。VPN 虚拟出来的网络处于网络层，路由规则一次性配好之后，就不需要再为了新的七层应用作额外的配置。
机器信息和 IP 规划 VPN 网段：192.168.2.0/24
bcc 的公网 IP（假设为）： 106.13.13.13
   机器 主机名 VPN IP 本地网卡 本地 IP     百度云机器 Ubuntu bcc 192.168.2.1/24 eth0 -   家庭内网机器 ArchLinux home 192.168.2.2/24 wlp2s0 192.168.0.9/24   公司内网机器 CentOS company 192.168.2.3/24 ens192 10.10.13.62/24   公司内网机器 CentOS target - ens192 10.10.13.61/24    Task 前提  公网服务器（bcc） 公司和家庭内网的机器能够访问公网（home 和 company 都可以访问到 bcc） 公司内网网段和家庭内网网段不冲突  目标 能从家庭网络访问公司内部网络（使用 wireguard + iptables + route路由表实现），如在本文中就是要实现 home 能够访问到 target</description>
    </item>
    
    <item>
      <title>通过实验学习 Linux VETH 和 Bridge</title>
      <link>https://gobomb.github.io/post/learning-linux-veth-and-bridge/</link>
      <pubDate>Tue, 17 Sep 2019 01:07:13 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/learning-linux-veth-and-bridge/</guid>
      <description>背景 docker 容器的网络，以及 flannel 网络的实现都用到了 VETH 和 Bridge 这两种虚拟设备。大致原理是在主机创建一个 Bridge，每当一个新的容器创建，就创建一对 VETH，一端连接到主机的 Bridge，另一端连接到容器命名空间里，作为容器的默认网卡，并将容器的默认路由设置为 Bridge。
本文将用 Linux 的相关命令，模拟上述网络创建的过程，并通过实验和抓包对原理进行验证，加深对容器和 Linux 网络的理解。
概念 VETH（virtual Ethernet）：是 Linux 内核支持的一种虚拟网络设备，表示一对虚拟的网络接口，VETH 对的两端可以处于不同的网络命名空间，所以可以用来做主机和容器之间的网络通信。
Bridge：Bridge 类似于交换机，用来做二层的交换。可以将其他网络设备挂在 Bridge 上面，当有数据到达时，Bridge 会根据报文中的 MAC 信息进行广播、转发或丢弃。
Namespace：是 Linux 提供的一种内核级别环境隔离的方法。不同命名空间下的资源集合无法互相访问。
实验环境： root@ubuntu:~# cat /proc/version Linux version 4.4.0-38-generic (buildd@lgw01-58) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) ) #57-Ubuntu SMP Tue Sep 6 15:42:33 UTC 2016 网卡名称和 IP 地址为ens160 10.10.12.27/24
网络拓扑示意图  +------------------------+ | | iptables +----------+ | br01 192.</description>
    </item>
    
    <item>
      <title>Kubernetes 网络学习：阅读 Flannel 源码</title>
      <link>https://gobomb.github.io/post/learning-k8s-networking-reading-flannel-source/</link>
      <pubDate>Mon, 09 Sep 2019 18:59:32 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/learning-k8s-networking-reading-flannel-source/</guid>
      <description>1. 背景：k8s 网络的介绍 k8s 的网络分几个层面：pod 网络、service 网络、node 网络。pod 网络一般由 flannel、weave、calico 这些 CNI 插件创建和维持，service 网络一般由 kube-proxy 通过操作 node 机器的 iptables 来维持，node 网络来自物理机或虚拟机层面的配置。
本文讨论的重点和范围是 pod 网络。
k8s 的 pod 网络有如下假设:
 每个 pod 有单独的 IP，所有 pod 都处于一张扁平的、可以互相 ping 通的网络上，即使 pod 处于不同的 node/虚拟机/宿主机上面。 一个 pod 的所有容器共享一个网络空间（网卡和IP）  为了让 pod 网络变得平坦，一般有两种方式：
 使用 overlay 网络； 使用路由协议（BGP）  overlay 是建立在实际物理网络上的一张虚拟逻辑网络，用二层、三层或四层协议来封装 pod 二层数据帧，与 VPN 有些类似，由于存在封包和拆包的过程，性能会有所损耗，flannel使用这种方式；使用路由协议，则 pod 和物理网络共同处于一张大三层网络，calico 使用这种方式。
本文将通过分析 flannel 源码(v0.11.0) 来学习 pod 网络的形成。
术语说明 本文假设读者对 k8s 的一些概念有所了解，对计算机网络二层和三层协议有基本的认识。</description>
    </item>
    
    <item>
      <title>容器运行时笔记</title>
      <link>https://gobomb.github.io/post/container-runtime-note/</link>
      <pubDate>Thu, 01 Aug 2019 08:14:03 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/container-runtime-note/</guid>
      <description>Kubernetes 通过容器运行时（container runtime）来启动和管理容器。官方文档列举了以下几种 runtime：Docker，CRI-O，Containerd，fraki。它们之间有什么区别和联系呢？经常会看到 OCI、CRI 这些缩写，这些和容器、docker 到底是什么关系呢？
这篇文章不会深入到很细节的部分，旨在为初学者提供一个比较初步的概览，对一些基本概念做一些简单介绍。
简单说说什么是容器。容器实际上是 Linux 内核几组功能的组合：cgroup、namespace 和 union file system。cgroup 用来限制进程组所使用的系统资源（CPU、Memory、IO 等）；namespace 用来隔离进程对系统资源的访问（IPC、Network、PID 等），让不同 namespace 的进程看不到彼此的存在；union file system 用来支持对文件系统的修改分层。
容器并不是虚拟机。虚拟机一般会虚拟完整的操作系统内核，而容器只是虚拟进程的运行环境。容器用到的技术，本身就是内核提供的。容器与容器是共享一个内核的，而虚拟机与虚拟机有可能跑在同一台物理机器上但是各有一个内核。
容器运行时是管理容器和容器镜像的程序。对于 k8s 而言，runtime 指的是 CRI-runtime，它不关心如何调用内核 API，只规定了 kubelet 与容器相关的接口；对于 docker 而言，runtime 一般指的是 ORI-runtime，封装具体的内核交互和系统调用。
OCI 标准 OCI（Open Container Initiative）标准是由 Docker 公司主导的一个关于容器格式和运行时的标准或规范，包含运行时标准（runtime-spec ）和容器镜像标准（image-spec）。运行时标准规定了怎么去运行一个容器，如何去表达容器的状态（state）和生命周期（lifestyle）、如何设置 namespace、cgroup、文件系统等等，可以理解为运行期的动态描述；而容器镜像标准规定了容器镜像的格式、配置、元数据等，可以理解为对镜像的静态描述。
为什么要搞这么一个标准呢？应该是为了防止各家容器各有一套互不兼容的格式导致生态过于碎片化，另外一个目的是尽管目前只有 Linux 系统有容器，但万一我们要在 Windows 或者 Unix 上实现容器，要不要重新搞一套标准呢？OCI 规范也可以在其他操作系统和平台上实现。
runc OCI 规范在 Linux 上的完整实现是 runC。我们通过 runC 命令可以看到一些基本的说明：
[root@master ~]# runc --help NAME: runc - Open Container Initiative runtime runc is a command line client for running applications packaged according to the Open Container Initiative (OCI) format and is a compliant implementation of the Open Container Initiative specification.</description>
    </item>
    
    <item>
      <title>2018 年终总结</title>
      <link>https://gobomb.github.io/post/2018-summarize/</link>
      <pubDate>Wed, 02 Jan 2019 03:26:33 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/2018-summarize/</guid>
      <description>2018 过去了，年终总结我拖到2019才写。
这一年，正式从校园进入职场。
18 年读的书特别少，小说基本不怎么读了，实在没有读小说的心境。现在能想起来的就是毛姆的《面纱》，读完不过瘾还去看了改编的电影。毛姆实在是一位很合我口味的小说家了，到目前为止他的书还没有让我失望过。
看了几本讲互联网企业和开源历史的书：吴军的《浪潮之巅》、Eric S. Raymond的《大教堂与集市》，让刚刚进入行业的我对整个 IT 产业的玩法有了比较宏观的认识。之前对 IT 的了解，仅仅停留砸在 To C、做应用、做网站的维度，实际上是比较狭隘的。
《浪潮之巅》把硅谷企业的创业史都过了一遍，勾勒出时代浪潮的发展轨迹，对其中的企业给出了一些盖棺定论的评价，这对我的知识体系是个很有益的补充。也让我大致了解了一下资本的游戏规则：股票期权是怎么运作的，金融危机的逻辑是什么，风险投资、投资银行是什么……而 18 年恰逢经济形势不好，与书中的内容对照，让人唏嘘。
《大教堂与集市》是介绍开源（open source）运动很著名的书了。慕名已久直到最近才读。对了解什么是开源，什么是黑客，什么是好的自由软件的开发模式很有帮助。Eric S. Raymond（ESR）算是开源的推广者，在这几十年间，参与开源、也见证开源概念的形成与发展。这本书也有对软件工程方法和组织架构的大量探讨。这本书出版于1999年，那时候还不甚确定的事情，在20年后的今天得到了不少印证。跟很多事情的发展一样，大厦不是一日建成的，我们认为理所应当的事情，在最开始并不是理所应当的。所以倒回去看一些前辈当时的努力和思考，很有必要。万一轮到我们这一辈去创造历史，也不是无前例可循。
实际上开源在不同的年代，在不同的人那里，有着不同的意义，比如在大胡子 Richard Stallman （RMS）和他倡导的自由软件运动（free software movement）那里，”开源“是激进且反商业的；相较之下 ESR 和 Linus Torvalds 就不会很排斥商业；而对像 Red Hat （近期被 IBM 收购也是一件值得关注的事了）这种靠开源成功上市的公司，开源是一门生意（国内的也有采用开源模式创业的公司 PingCAP）。所以我们可以看到现在有各种各样的开源协议，可以说每种协议背后都有不同的价值观和商业考虑。实际上开源在今天已经是主流了，连微软这种传统上“邪恶”的闭源公司，都已经在开源贡献列表里排得上名号了，甚至收购了开源社区 Github。开源在未来会有怎样的发展，我很期待。
Linux Kernel 是开源软件里必须提到的成果了，现在能接触到的（Web）服务器上面，跑的都是 Linux。但实际上 Linux 的全称应该是 GNU/Linux 才对，GNU（GNU&amp;rsquo;s Not Unix!） 本来是自由软件基金会想要搞出来的完全自由的操作系统，他们开发了一堆与 Unix 兼容的程序，但是他们的内核 Hurd 没有成功，反而后来居上的 Linux Kernel 加上 GNU 那些自由软件流行起来。Linux Kernel 与 Hurd 之间的技术差异（宏内核和微内核），也有很多好玩的技术细节。（这段历史我是看了老板推荐的两部纪录片《The Code》《Revolution OS》了解到的。也算补了一课。）
我对开源产生兴趣，一个原因是我很认可自由、创造性的价值，另一方面，我也关心技术如何影响社会，如何成为一种“运动”。技术可以是专制、贬损人类的工具，也可以用来增进人类福利与自由，而这不是单纯的技术问题了。开源从直觉上讲，似乎是和逐利的商业互相矛盾的，也不符合人自私的天性，但实际上它激发了人的创造力，使得软件开发离不了开源。资本社会，大可见行业壁垒、信息不对称、知识产权垄断，互联网在走向封闭。我觉得开源这样一种分布式的、扁平的、自由的模式，是有希望成为我们与资本、权力抗衡的武器的。
我很喜欢这本书附录的一篇文章：《如何成为一名黑客》，读完十分兴奋，相见恨晚，推荐给了很多朋友。读完意犹未尽，我还又去重读了一遍 Paul Graham 的《黑客与画家》。这算是看书的乐趣之一了：原来自己思考和选择的结论，那些有经验的写作者也是这么认为的，我能从他们那里得到了印证，我并不是孤独的。而我想错的部分，也可以得到修正。我在亲身试错之前，能够“提前”得到反馈。我以后的行动，也可以建立在前人的真知灼见上。</description>
    </item>
    
    <item>
      <title>ngrok 源码解析</title>
      <link>https://gobomb.github.io/post/ngrok-source-code-reading/</link>
      <pubDate>Mon, 17 Dec 2018 02:08:25 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/ngrok-source-code-reading/</guid>
      <description>背景 ngrok是我第一个完整阅读过源码的开源项目。一开始接触这套代码我几乎还是 go 语言零基础，之前只写过一点点 Web 后端 API，后来趁着做毕业设计的机会还跟着源码重新敲了一遍，按照我自己的需要小改了一下。所以我从中收获了不少东西，一直都想写一篇文章总结一下我对这套代码的理解。
ngrok 的目的是将本地的端口反向代理到公网，让你可以通过访问公网某个服务器，经过流量转发，访问到内网的机器。这个事情你可以有不同的叫法：反向代理、端口转发（映射）、内网穿透1……原理其实不难，解决的需求也简单。
我们个人的设备一般都在 NAT （公司、学校内网，运营商也会喜欢做 NAT）后面，不能被公网设备直接访问到。内网机器可以主动向公网发起连接，但公网却不能穿越 NAT 主动访问到内网机器。ngrok 就适合用来穿越 NAT 来临时暴露内网服务。在技术人员常聚集的社区 V2EX，经常能看到问怎么做内网穿透的月经贴。可见这个需求十分常见。
内网穿透的工具，现在大家用的比较多的是 frp了。frp 是中国人开发的，增加了很多新的功能，使用体验也比 ngrok 好很多，社区比较活跃。ngrok 虽然出现得比较早，但 2.0 转闭源， 1.0 不再维护了，用得人也少了。但这不妨碍我们扒源码学习。
项目结构 先来看一下 ngrok 整个项目的结构（忽略掉了一些不必要的文件）：
. ├── ... ├── assets	// 存放 web 静态文件和 tls 文件 │ ├── ... └── src └── ngrok ├── cache	// 缓存 │ └── lru.go ├── client │ ├── cli.go	// 命令行参数定义 │ ├── config.go	// 配置文件读取 │ ├── controller.</description>
    </item>
    
    <item>
      <title>Arch Linux 安装记录</title>
      <link>https://gobomb.github.io/post/install-arch-linux/</link>
      <pubDate>Sun, 09 Sep 2018 16:22:20 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/install-arch-linux/</guid>
      <description>旧笔记本之前是安装着 Windows 10 的，自从工作开始使用 rMBP 之后，就闲置着。我想不如重装成 Linux 系统，当作一个私人服务器用，也可以加深一下对操作系统的理解。很早就听说滚动发行的 Arch Linux 的大名，就想趁机试一试。跟着官方 Wiki 走，大概花了6、7个小时才装好，中间也遇到了一些问题，但那时没有记录下来。用了一段时间后，/boot分区不小心被我覆盖了（估计是我使用 lvm 创建物理卷的时候把引导分区给格式化了），导致系统启动不了，一时半会也不会修。
放着有半个月，今天有空干脆格盘重装，再过一遍安装过程，把过程和问题记录下来，下一次遇到问题就不用再去 google。覆盖引导的问题，也是因为我没有把分区规划的信息留下来，后面自己也乱了。而且实际上90%的问题都会重复遇到，做好记录能极大提高效率。
Arch 官方 Wiki 做得真是很好，基本上遇到问题耐心读一读 Wiki 就能够解决。用来学习 Linux 相关知识也特别有用。
整个安装过程的大体步骤
 制作安装介质 从安装介质启动 分区和挂载磁盘 下载安装基本包到系统分区中 从安装介质切换到系统 完成基本的设置，并安装引导  分区和安装引导的部分是比较容易出错的，但大部分情况都可以从 Arch Wiki 中找到答案。
安装准备   下载 ArchLinux iso 文件
https://www.archlinux.org/download/
  制作安装介质
  在 Mac 下安装pv （用来查看dd的进度）
brew install pv
  找到 U 盘对应的设备
diskutil list
我这里是/dev/disk2
  查看 U 盘挂载的目录</description>
    </item>
    
    <item>
      <title>Cheat Sheet</title>
      <link>https://gobomb.github.io/post/cheatsheet/</link>
      <pubDate>Sun, 18 Mar 2018 14:02:01 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/cheatsheet/</guid>
      <description>Linux   安装 deb 文件：
sudo dpkg -i [name].deb
sudo apt-get install -f （修复依赖关系）
  linux 查找文件
find [path] -name &amp;quot;*.log&amp;quot; 
  ubuntu（桌面版） 在终端输入xkill，鼠标变成x，点击 GUI 程序，可强制关闭之
  调用自己定义的编辑器来编辑当前命令行:
ctrl-x ctrl-e
设置自己的默认编辑器：
export EDITOR=vi
  shell 脚本报错：... /bin/sh^M: bad interpreter: No such file or directory
解决：
vim [name.sh]
:set ff 查看文件格式，若看到输出fileformat=dos，可:set ff=unix修改为 unix 格式
  本地文件传输到远程机器
scp -P [port] /path/to/source [user]@[ip]:/path/to/target
  不挂断地运行命令</description>
    </item>
    
    <item>
      <title>编程语言的求值策略</title>
      <link>https://gobomb.github.io/post/evaluation_strategy/</link>
      <pubDate>Thu, 25 Jan 2018 17:09:03 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/evaluation_strategy/</guid>
      <description>在面试的时候遇到一个问题：“golang 的传参是按值传递还是按引用传递？”我第一反应是 go 在很多场景下传参和赋值都会发生内存的复制，同时记得 go 里也有引用类型（map、slice、channel），就贸然给出“类似 slice 的引用类型是按引用传递的，其他是按值传递”的错误回答（正确答案是“golang 都是按值传递”）。
这其实是与求值策略(Evaluation strategy)相关的概念。在传递参数的时候，编译器是怎么进行求值的，是否会发生内存的复制，不同的语言有自己的规定。不了解所使用的编程语言的规定，就很容易出错，也很容易写出低效率的代码。
定义 这些概念有些抽象，从字面理解，很容易产生歧义。先尽量跳出某一种编程语言的习惯，做出一些定义，再来讨论具体语言的特定做法。了解通用的定义，对于不同的语言的规定以及为何这么规定会更加清晰。
求值策略(Evaluation strategy)指确定编程语言中表达式的求值的一组（通常确定性的）规则。描述的是求值和传值的方式，关注的点在于，表达式在调用函数的过程中，求值的时机、值的形式的选取等问题。
 按值传递（Pass by value）：函数的形参是被调用时所传实参的副本。修改形参的值并不会影响实参。（发生了值的复制） 按引用传递（Pass by reference）：传递给函数的是它的实参的隐式引用（别名）而不是实参的拷贝。修改形参会改变实参。（发生了引用的复制） 按共享对象传递（Pass by sharing）：传一个共享对象的引用的副本。修改形参的值会影响实参，修改形参本身不会影响实参。（发生了地址/指针的复制）  比如传递一个a，设a的引用为rf，a的地址为ad：
  按值传递：a的值复制给b，函数拿到的是b的值和b的引用，和a无关。函数通过b的引用修改b，对调用者不可见。
 （主函数）rf of a -&amp;gt; ad of a -&amp;gt; a:100 | | 复 ↓ 制 （子函数）rf of b -&amp;gt; ad of b -&amp;gt; b:100 ``
  按引用传递：a的值没有发生复制，函数拿到的是a的引用ar，通过这个引用修改a，也对调用者可见。
 （主函数）rf of a -&amp;gt; ad of a -&amp;gt; a:100 |	^ 复 |	| 制 ↓	| （子函数）rf of a ----—--&amp;gt; ``</description>
    </item>
    
    <item>
      <title>2017 年终总结</title>
      <link>https://gobomb.github.io/post/2017-summarize/</link>
      <pubDate>Sun, 31 Dec 2017 20:48:34 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/2017-summarize/</guid>
      <description>一晃眼一年又过去了。再过半年我就要毕业了。
教训 今年找到第一份实习，学一门新的编程语言。实习的时候几乎是零基础，遇到什么问题就问上网查、问同事，把事情勉强做成。在这个过程中，越发觉得知识要成体系。以前我总以为，记忆性的东西不必死记硬背，比如 Linux 某个命令的用法，某个网络协议的细节等，知道去哪里寻找答案，能够解决当前的问题，然后做好记录就够了。但问题在于，这样每次临时去搜索，其实是很耗费时间的。而且也不一定保证能够找到想要的信息。试图走捷径，不踏踏实实理解和记忆，最后其实得不偿失。
今年看的一本书《构建之法》提到一个观点：“技能的反面”是“解决（低层次）问题”，读到这一章的时候，真是醍醐灌顶。原来我一直在解决“低层次”问题。
低层次问题理应烂熟于心，才能把主要时间精力集中在高层次问题上。把技能磨练到精通的程度，才能够看得更远。正确的做法是体系化地学习技能或知识，该训练的训练扎实，把招式内化为本能反应。轻视招式是很愚蠢的。我在这里吃了大亏，面试的时候好几次被问起：作为计算机科班学生，为什么连基本的数据结构、数据库都那么不熟练？感觉把母校的脸丢光了。
关于面试 秋招并不顺利，除了基础不够扎实外，自己也没有拿得出手的项目。另外面试经验不足也是原因，很容易紧张，没有注意表达方式。有时候即使知道对方需要的答案，但因为自己没有表达好，反而会成为劣势。STAR 法则——情境（situation）、任务（task）、行动（action）、结果（result）——还是很有道理的。要有意识地训练自己的思维和表达。
我不太适应电话交流。书面表达和当面交流的经验比较多。书面表达我可以反复斟酌修改，而当面交流，可以得到语言之外的反馈，比如肢体语言、眼神，我很依赖这些反应。在电话里，只有唯一的媒介，声音，而且必须立即给出反应，这让我很有压力。我第一次电话面试时，大脑卡壳，陷入长时间的沉默，十分尴尬。而这沉默也是无效的，因为我没有办法让自己思考，只是浪费双方的时间。
朋友建议我电话面试的时候可以准备纸笔，在面试的过程中写写画画，整理一下思路，搭一个大致的框架。我觉得有道理，纸就像一个高速缓存，大脑的缓存不够，就再建立一个二级缓存，加速数据传输。动起手来先把知识从大脑的长期记忆区（硬盘）取出来，提高命中率。面试的目的也是看你的长期记忆里有没有他需要的能力，我所做的就是把能力有效地展现出来。要尽量往消除信息不对称的方向走。这也有助于把注意力从“声音/沉默”转移到问题本身。
重点就是要表达出来，把自己懂的都说清楚，说对说错都没有关系，但如果不说出来对方永远不知道你的水平如何，也就没有给 offer 的充分理由了。
这也是为何技术招聘要看 Github 页面和技术博客，以及注重白板编程的原因。大三选修了一门创新短课——区块链和数字货币。有个工作量证明(Proof Of Work，POW)的概念，比特币的 POW 就是暴力计算散列函数，矿工投入必要的计算资源去碰撞出某个值，得到这个值就证明了矿工付出了相应的“工作量”，有资格得到奖励。这是个很巧妙的设计，从数学上保证了人很难作弊，而无需一个第三方和中心化的机构来做出担保。面试需要的也是“工作量证明”，面试者需要证明自己有相应的能力，开源项目、博客和白板编程都是过往工作量的证明，虽然这种证明的效力比不上比特币那种数学的方式，但道理是类似的。开源项目、博客和白板编程，都是看得见的产出。而产出会倒逼程序员主动去学习和进步。
后端：总结和计划 开始确定自己要走后端路线。比起前两年，对如何学习这回事开始有一点谱。似乎醒悟得有点晚，但俗话说“种一颗树最好的时机是十年前，其次是现在”，只要开始行动就不算晚。
我是今年才明确自己的方向的。上半年实习，开始学 Go 语言，接触 Web 开发，学用框架，看懂开源项目 ngrok 的大致逻辑。下半年都在复习基础知识，计算机网络、操作系统，进一步深入 Go 语言。十月份开始写博客，在 Github 上试着开发和完善一个项目。
个人技术栈不管广度还是深度，都不够。但是急不来，还是得踏踏实实写代码，多总结。总结很重要，总结是个系统化的过程，把碎片化的经验组织起来，纳入自身的工具箱。能达到可复用的效果。这跟写代码是一个道理，做到高内聚低耦合，功能模块做好拆分。遇到具体的情景和需求，能够快速高效地调用。
接下来的打算是，做毕业设计，更新博客，补基础（特别是数据结构和数据库），继续面试。
其他 实习时交到了新的朋友，参与了个外包项目，赚了点零花钱。
准备秋招的时候，和舍友互相学习，讨论问题，收获也很大。
大学待过的两个社团——《深大青年》杂志社、学子天地——都被校团委解散重组了，很可惜。迷茫的时候深青同仁给了我不少支持，学子的技术大佬帮我看了简历，给了我很中肯的建议。如果没有社团，大学生活可能会完全不一样。
给野人公众号写了几篇读书笔记，帮野人杂志第三期做了校对。
看了几本技术书，都是囫囵吞枣地过一遍，读得不是很仔细……有点消化不良。怎么看技术书比较高效，我还在摸索中。</description>
    </item>
    
    <item>
      <title>交叉编译 V2Ray</title>
      <link>https://gobomb.github.io/post/cross-complie-v2ray/</link>
      <pubDate>Fri, 29 Dec 2017 17:17:27 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/cross-complie-v2ray/</guid>
      <description>问题和背景 朋友想在路由器里跑 V2Ray，但官方 release 版本不能用，拜托我重新编译一份可执行文件。该路由器没有 FPU（Float Point Unit，浮点运算单元，专用于浮点运算的处理器)，官方 Mips 版本并不支持软解浮点数运算，无法顺利运行。
环境  目标机器：OpenWRT mipsle（32位机器，小端序） 本地机器：CentOS amd64 golang1.9  解决过程 参考官方提供的编译步骤：
1. （安装 git 和 golang 环境的步骤略去） 2. 下载 V2Ray 源文件：`go get -u v2ray.com/core/...` 3. 下载 V2Ray 扩展包：`go get -u v2ray.com/ext/...` 4. 生成编译脚本： `go install v2ray.com/ext/tools/build/vbuild` 5. 编译 `V2Ray：$GOPATH/bin/vbuild` 6. V2Ray 程序及配置文件会被放在 `$GOPATH/bin/v2ray-XXX` 文件夹下（XXX 视平台不同而不同）   编译 V2Ray 至少要 go1.9 以上的版本 最新的 v2ray 源码是 go1.9 实现的，如果用 go1.8 进行编译会提示找不到“math/bits”标准库文件 unrecognized import path &amp;quot;math/bits&amp;quot; (import path does not begin with hostname) （https://github.</description>
    </item>
    
    <item>
      <title>并发模型比较</title>
      <link>https://gobomb.github.io/post/high-concurrency-model/</link>
      <pubDate>Sat, 25 Nov 2017 22:13:27 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/high-concurrency-model/</guid>
      <description>Go 的特色之一就是 goroutine ，使得程序员进行并发编程更加方便，适合用来进行服务器编程。作为后端开发工程师，有必要了解并发编程面临的场景和常见的解决方案。一般情况下，是怎样做高并发的编程呢？有那些经典的模型呢？
 一切始于 C10k C10k 就是 Client 10000，单机服务器同时服务1万个客户端。当然，现在的业务面临的是 C100k、C1000k 了。早期的服务器是基于进程/线程模型，每新来一个连接，就分配一个进程（线程）去处理这个连接。而进程（线程）在操作系统中，占有一定的资源。由于硬件的限制，进程（线程）的创建是有瓶颈的。另外进程（线程）的上下文切换也有成本：每次调度器调度线程，操作系统都要把线程的各种必要的信息，如程序计数器、堆栈、寄存器、状态等保存起来。
CPU 运算远远快于 I/O 操作。一般而言，常见的互联网应用（比如 Web）都是 I/O 密集型而非计算密集型。I/O 密集型是指，计算机 CPU 大量的时间都花在等待数据的输入输出，而不是计算。当 CPU 大部分时间都在等待 I/O 的时候，大部分计算资源都被浪费掉了。
显然，简单粗暴地开一个进程/线程去 handle 一个连接是不够的。为了达到高并发，应该好好考虑一下 I/O 策略。同样的硬件条件下，不同的设计产生的效果差别也会很大。在讨论几种 I/O 模型之前，先介绍一下同步/异步、阻塞/非阻塞的概念，以及操作系统的知识。
参考：
The C10K problem
 同步/异步？阻塞/非阻塞？ 同步，是调用者主动去查看调用的状态，实时跟进，不可以同时做其他事情；异步，则是其他线程（可能是被调用者，也可能是一个帮你关注调用结果的实体）来通知调用者，调用者可以同时做其他事情，当调用结果更新，调用者会收到通知，再去处理调用结果。
主动轮询就是同步行为，调用者需要不断地去查看状态是否更新，在状态更新或结果返回之后，调用者才能跳出循环。而注册回调函数的方式，则是异步的。在 Web 应用里，前端可通过 jQuery AJAX 以异步的方式向服务器请求数据，在数据还没到达之前，AJAX 里的回调函数不会被执行，而调用者执行 AJAX 之后的代码，等到数据到达，才触发回调里的逻辑。
（感谢 @CuberL 在评论中指出: jQuery AJAX 也有同步和异步两种方式。可通过指定 async 的值来确定，默认情况下是异步方式。）
用一个生活中的例子来说明：快递员（调用者）送包裹，打电话叫你过来取（调用），然后他会等在那里，实时跟进包裹是否被你拿走（主动查看调用状态），只有你来拿了（调用状态更新）他才能离开，并记下此包裹被取（处理调用结果），继续送其他包裹，这就是同步方式。异步的情况则是，快递员打电话叫你过来取（调用），把包裹放在快递柜里（向另一个执行实体注册回调函数），然后离开，做其他事情。你从快递柜里取走包裹（调用状态更新），快递柜通知快递员或者你打电话告诉快递员：包裹已经被取走了（把状态更新的信息通知调用者），快递员可以停下其他事情，记录一下此包裹被取（执行回调函数，也即处理调用结果），继续做其他事情。
再总结一下，调用是同步方式还是异步方式，就是看对于调用的状态变更这个信息，调用者是主动获取的还是被动获取的。
阻塞和非阻塞的区别是调用后被调用者是否立即返回，调用者是不是在等待。 A 调完 B，就在调用处等待（阻塞），直到 B 方法返回才继续执行剩下的代码，这就是阻塞调用。而非阻塞是 A 方法调用 B 方法，B 方法立即返回，A 可以继续执行下面的代码，不会等在这里，不会被该调用阻塞。当某个方法被阻塞了，该方法所在的线程会被挂起，被操作系统的调度器放到阻塞队列，直到 A 等待的事件发生，才从阻塞态转到就绪态。</description>
    </item>
    
    <item>
      <title>Golang 自带的单元测试</title>
      <link>https://gobomb.github.io/post/how-to-write-unit-testing-in-golang/</link>
      <pubDate>Mon, 06 Nov 2017 16:52:19 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/how-to-write-unit-testing-in-golang/</guid>
      <description>为什么要写单元测试 以前写程序的时候，一般不写测试，阅读开源代码遇到测试也都是跳过不读。调试的时候一半都是手动输入测试数据，在代码里打印 log 信息。实际上重复性的工作很多，这一部分是可以用单元测试来做的。
另外，当项目比较大的时候，一般都是把项目分割成几个模块来写的。可以分别保证各个模块的正确性，最后再把各个项目组合起来。这时候也需要单元测试，增强可维护性。
人的记忆力和思考能力毕竟是有限的，并不一定能马上想到边界条件和 bug 可能出现的地方，当代码发生更改，边界条件可能就改变了，程序可能会跑不通，这时跑一下测试代码，可以更快发现问题。
测试保证程序是可运行的，运行结果是正确的，使问题及早暴露，便于问题的定位解决。而性能测试则关注程序在高并发的情况下的稳定性。
单元测试也可以方便读代码的人读懂，通过测试代码可以更快了解这个项目到底是干嘛的、该如何用。
怎么写单元测试 单元测试   go语言自己有一个轻量级的测试框架 testing和命令go test,可用来进行单元测试和性能测试。
  测试文件用 xxx_test.go命名，测试函数命名为TestXxx或Test_Xxx
  在终端中输入 go test，将对当前目录下的所有xxx_test.go文件进行编译并自动运行测试。
  测试某个文件，要带上被测试的原文件
   go test xxx.go xxx_test.go
  测试某个方法:go test -run=&#39;Test_Xxx&#39;
  go test -v 则输出通过的测试函数信息
  如对以下代码进行测试：
package test func Fibonacci(n int) int{ if n==1{ return 1 }else if n==0{ return 1 } return Fibonacci(n-1)+Fibonacci(n-2) } 测试代码为：</description>
    </item>
    
    <item>
      <title>用 Hugo 和 GitHub Page 搭建博客</title>
      <link>https://gobomb.github.io/post/make-a-blog-by-hugo-and-github-page/</link>
      <pubDate>Tue, 31 Oct 2017 16:02:19 +0800</pubDate>
      
      <guid>https://gobomb.github.io/post/make-a-blog-by-hugo-and-github-page/</guid>
      <description>图省事，懒得花钱买域名和备案，就用 GitHub Page 来搭博客了。
hugo 是用 Golang 写的静态网站生成器。支持 Markdown 语法。 另一个用js写的同类工具jekyll 比较出名。安装和使用Hugo  如果有 Go 环境，直接在终端输入：  go get -v github.com/spf13/hugo hugo new site /path/to/site   path/to/site是本地站点目录
  创建 about 页面：
  hugo new about.md  皮肤我用的是 rockrock：  cd themes git clone https://github.com/chingli/rockrock 在本地运行   在站点根目录下运行： hugo server --theme=rockrock --buildDrafts   在浏览器访问： http://localhost:1313   这里修改 Markdown 文件可以动态更新，很方便。   在 GitHub 上部署 1 在 GitHub 建一个 Repository，命名为 sidddhartha.</description>
    </item>
    
  </channel>
</rss>
